# Walmart Sales ETL Pipeline

##  Overview
This project implements a complete **ETL pipeline** for Walmart sales data using **Apache Airflow**.

The pipeline is designed as a portfolio project to demonstrate practical **data engineering skills**:
- Building modular ETL workflows  
- Orchestrating jobs with **Airflow**  
- Persisting data in **PostgreSQL**  
- Containerizing with **Docker Compose**  
- Adding CI/CD automation with **GitHub Actions**  
- (Optional) Visualizing metrics with **Grafana**  

---

##  Project Structure
```
â”œâ”€â”€ airflow/                 # Airflow components
â”‚   â”œâ”€â”€ dags/                # DAG definitions
â”‚   â”‚   â””â”€â”€ dag_walmart.py
â”‚   â”œâ”€â”€ etl/                 # ETL modules
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ quality.py
â”‚   â”œâ”€â”€ sql/                 # SQL scripts for transformations & metrics
â”‚   â”œâ”€â”€ data/                # Source dataset
â”‚   â”‚   â””â”€â”€ walmart.csv
â”‚   â””â”€â”€ tests/               # Unit tests
â”œâ”€â”€ db/                      # PostgreSQL init scripts
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ grafana/                 # Grafana provisioning (optional)
â”œâ”€â”€ docs/                    # Documentation & demos
â”‚   â””â”€â”€ screenshots/         # UI results
â”‚       â”œâ”€â”€ airflow_ui.png
â”‚       â””â”€â”€ grafana_dashboard.png
â”œâ”€â”€ docker-compose.yml       # Service orchestration
â”œâ”€â”€ Dockerfile               # Custom Airflow image
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md

```

---

##  Tech Stack
- **Apache Airflow** â€“ orchestration  
- **PostgreSQL** â€“ database  
- **Docker / Docker Compose** â€“ containerization  
- **GitHub Actions** â€“ CI/CD  
- **Pytest** â€“ testing  
- **Grafana** â€“ visualization  (optional)

---

##  Getting Started

### 1. Clone repository
```bash
git clone https://github.com/dravindel/ecommerce-etl.git
cd ecommerce-etl
```

### 2. Create `.env` file
```env
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
AIRFLOW__CORE__FERNET_KEY=YOUR_KEY
```

### 3. Start services
```bash
docker-compose up --build -d
```

Access after startup:
- Airflow UI â†’ [http://localhost:8080](http://localhost:8080)  
- Grafana (optional) â†’ [http://localhost:3000](http://localhost:3000)  

---

##  ETL Pipeline

Steps in the DAG:
1. **Extract** â†’ Load raw data (CSV â†’ PostgreSQL).  
2. **Data Quality** â†’ Validate input.  
3. **Load Raw** â†’ Store unprocessed data.  
4. **Transform** â†’ Clean & aggregate.  
5. **Load Processed** â†’ Persist transformed data.  
6. **Metrics** â†’ Calculate KPIs.  

DAG flow:
```mermaid
flowchart TD
    A[Extract] --> B[Data Quality]
    B --> C[Load Raw]
    C --> D[Transform]
    D --> E[Load Processed]
    E --> F[Calculate Metrics]
```

---

##  Tests
Run unit tests locally:
```bash
pytest airflow/tests/ --disable-warnings -q
```

---

##  CI/CD
- **CI (ci.yaml):** runs linting (flake8) and tests (pytest).  
- **CD (cd.yaml):** builds & pushes Airflow Docker image.  

To enable CD, add the following secrets in your GitHub repo:
- `DOCKER_USERNAME`  
- `DOCKER_PASSWORD`  

---

## Demo illustration

### Airflow DAG
![Airflow DAG](docs/screenshots/airflow_ui.png)

### Grafana Dashboard
![Grafana Dashboard](docs/screenshots/grafana_dashboard.png)

---

## Author
**ðŸ‘¤ [Dravindel](https://www.linkedin.com/in/dmitrylakhov)**  
Portfolio project for **Data Engineering interviews demonstrations**.

