# Walmart Sales ETL Pipeline

##  Overview
This project implements a complete **ETL pipeline** for Walmart sales data using **Apache Airflow**.

The pipeline is designed as a portfolio project to demonstrate practical **data engineering skills**:
- Building modular ETL workflows  
- Orchestrating jobs with **Airflow**  
- Persisting data in **PostgreSQL**  
- Containerizing with **Docker Compose**  
- Adding CI/CD automation with **GitHub Actions**  
- Visualizing metrics with **Grafana**  

---

## Architecture

**Data Flow:**

```mermaid
flowchart LR
    RAW[Raw Layer] --> STG[Staging Layer]
    STG --> PROCESSED[Processed Layer]
    PROCESSED --> METRICS[Metrics/KPIs]
```
- Raw Layer: Stores unprocessed, source data.
- Staging Layer (optional): Cleans, deduplicates, and enriches data for analytics. Can be implemented as an intermediate table (stg.orders) or handled in Python/SQL transformations before the processed layer.
- Processed Layer: Final business-ready tables, including calculated metrics and KPIs. This layer is ready for analytics, reporting, and dashboard consumption.
- Metrics Layer:Aggregated KPIs for dashboards and reporting. Typically stored in a separate table or materialized view to optimize queries for visualization tools like Grafana.


##  Project Structure
```
walmart-etl-pipeline/
â”œâ”€â”€ airflow/                         # Airflow components
â”‚   â”œâ”€â”€ dags/                        # DAG definitions
â”‚   â”‚   â”œâ”€â”€ __pycache__/             # Compiled Python files
â”‚   â”‚   â””â”€â”€ dag_walmart.py
â”‚   â”œâ”€â”€ etl/                         # Python ETL modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ quality.py
â”‚   â”œâ”€â”€ data/                        # Source CSV files
â”‚   â”‚   â””â”€â”€ walmart.csv
â”‚   â””â”€â”€ tests/                       # Unit and integration tests
â”‚       â”œâ”€â”€ data_quality.py
â”‚       â””â”€â”€ test_etl.py
â”œâ”€â”€ sql/                             # SQL scripts
â”‚   â”œâ”€â”€ transform.sql                # Transformations for processed layer
â”‚   â””â”€â”€ metrics.sql                  # KPI / aggregate calculations
â”œâ”€â”€ db/                              # Database initialization scripts
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ grafana/                         # Dashboards setup
â”œâ”€â”€ docs/                            # Documentation and demos
â”‚   â””â”€â”€ screenshots/
â”‚       â”œâ”€â”€ airflow_ui.png
â”‚       â””â”€â”€ grafana_dashboard.png
â”œâ”€â”€ requirements-airflow.txt         # Python dependencies for Airflow
â”œâ”€â”€ docker-compose.yml               # Service orchestration
â”œâ”€â”€ Dockerfile                       # Custom Airflow image
â””â”€â”€ README.md                        # Project documentation

```

---

##  Tech Stack
- **Apache Airflow** â€“ orchestration  
- **PostgreSQL** â€“ database  
- **Docker / Docker Compose** â€“ containerization  
- **GitHub Actions** â€“ CI/CD  
- **Pytest** â€“ testing  
- **Grafana** â€“ visualization

---

##  Getting Started

### 1. Clone repository
```bash
git clone https://github.com/dravindel/ecommerce-etl.git
cd ecommerce-etl
```

### 2. Create `.env` file
```env
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
AIRFLOW__CORE__FERNET_KEY=YOUR_KEY
```

### 3. Start services
```bash
docker-compose up --build -d
```

Access after startup:
- Airflow UI â†’ [http://localhost:8080](http://localhost:8080)  
- Grafana (optional) â†’ [http://localhost:3000](http://localhost:3000)  

---

##  ETL Pipeline

Steps in the DAG:
1. **Extract** â†’ Load raw data (CSV â†’ PostgreSQL).  
2. **Data Quality** â†’ Validate input.  
3. **Load Raw** â†’ Store unprocessed data.  
4. **Transform** â†’ Clean & aggregate.  
5. **Load Processed** â†’ Persist transformed data.  
6. **Metrics** â†’ Calculate KPIs.  

DAG flow:
```mermaid
flowchart TD
    A[Extract] --> B[Data Quality]
    B --> C[Load Raw]
    C --> D[Transform]
    D --> E[Load Processed]
    E --> F[Calculate Metrics]
```

---

##  Tests
Run unit tests locally:
```bash
pytest airflow/tests/ --disable-warnings -q
```

---

##  CI/CD
- **CI (ci.yaml):** runs linting (flake8) and tests (pytest).  
- **CD (cd.yaml):** builds & pushes Airflow Docker image.  

To enable CD, add the following secrets in your GitHub repo:
- `DOCKER_USERNAME`  
- `DOCKER_PASSWORD`  

---

## Demo illustration

### Airflow DAG
![Airflow DAG](docs/screenshots/airflow_ui.png)

### Grafana Dashboard
![Grafana Dashboard](docs/screenshots/grafana_dashboard.png)

---

## Author
**ðŸ‘¤ [Dravindel](https://www.linkedin.com/in/dmitrylakhov)**  
Portfolio project for **Data Engineering interviews demonstrations**.

